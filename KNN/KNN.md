# KNN(k 最邻近分类算法)
## 1.算法思路通过计算每个训练样例到待分类样品的距离，取和待分类样品距离最近的 K 个训练样例，K 个样品中哪个类别的训练样例占多数，则待分类样品就属于哪个类别 核心思想:如果一个样本在特征空间中的 k 个最相邻的样本中的大多数属于某一个类别，则该样本 也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或 者几个样本的类别来决定待分样本所属的类别。 kNN 方法在类别决策时，只与极少量的相邻样本 有关。由于 kNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的， 因此对于类域的交叉或重叠较多的待分样本集来说，kNN 方法较其他方法更为适合。## 2.算法描述1. 算距离:给定测试对象，计算它与训练集中的每个对象的距离依公式计算 Item 与 D1、D2 ... ...、Dj 之相似度。得到 Sim(Item, D1)、Sim(Item, D2)... ...、 Sim(Item, Dj)。2. 将Sim(Item,D1)、Sim(Item,D2)......、Sim(Item,Dj)排序，若是超过相似度阈值t则放入邻居 案例集合 NN。找邻居:圈定距离最近的 k 个训练对象，作为测试对象的近邻3. 自邻居案例集合NN中取出前k名，依多数决，得到Item可能类别。做分类:根据这 k 个近邻归属的主要类别，来对测试对象分类

## 3.算法步骤* step.1---初始化距离为最大值* step.2---计算未知样本和每个训练样本的距离 dist* step.3---得到目前 K 个最临近样本中的最大距离 maxdist* step.4---如果 dist 小于 maxdist，则将该训练样本作为 K-最近邻样本* step.5---重复步骤 2、3、4，直到未知样本和所有训练样本的距离都算完* step.6---统计 K-最近邻样本中每个类标号出现的次数* step.7---选择出现频率最大的类标号作为未知样本的类标号 该算法涉及 3 个主要因素:训练集、距离或相似的衡量、k 的大小。

## 4.算法优缺点 
### 1) 优点1.  简单，易于理解，易于实现，无需估计参数，无需训练;2.  适合样本容量比较大的分类问题3.  特别适合于多分类问题(multi-modal,对象具有多个类别标签)，例如根据基因特征来判断其功能分类，kNN 比 SVM 的表现要好

### 2) 缺点1. 懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢;
2. 可解释性较差，无法给出决策树那样的规则
3. 对于样本量较小的分类问题，会产生误分

## 5.常见问题### 1)K 值设定为多大k 太小，分类结果易受噪声点影响;k 太大，近邻中又可能包含太多的其它类别的点。(对距离加 权，可以降低 k 值设定的影响)k 值通常是采用交叉检验来确定(以 k=1 为基准)经验规则:k 一般低于训练样本数的平方根### 2)类别如何判定最合适
投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法 更恰当一些。### 3)如何选定合适的距离衡量
高维度对距离衡量的影响:众所周知当变量数越多，欧式距离的区分能力就越差。 变量值域对距离的影响:值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行 标准化。### 4)训练样本是否要一视同仁在训练集中，有些样本可能是更值得依赖的。 可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。 
### 5)性能问题kNN 是一种懒惰算法，平时不好好学习，考试(对测试样本分类)时才临阵磨枪(临时去找 k 个近 邻)。
懒惰的后果:构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并 计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量等。### 6)能否大幅减少训练样本量，同时又保持分类精度? 
浓缩技术(condensing)编辑技术(editing)

## 电影分类

|电影名称|打斗次数| 接吻次数 |电影类型|
| --- | --- | --- | --- ||California Man|3|104|Romance|He’s Not Really into Dudes|2|100|Romance||Beautiful Woman|1|81|Romance||Kevin Longblade|101|10|Action||Robo Slayer 3000|99|5|Action|
|Amped II|98|2|Action||未知|18|90|Unknown|


